---
title: 八股文
date: 2024-02-27 19:16:58
tags: 求职
---

# 一、java基础

## 8种基本数据类型

byte（8位，默认0B）、short（16位，默认0）、int（32位，默认0）、long（64位，默认0L）、float（32位，默认0.0f）、double（64位，默认0.0d）、char（16位，默认值\u0000）、boolean（jvm实现相关，一般情况下为1个字节，默认false）

## **包装类型的常量池技术** 

Byte、Short、Integer、Long这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据。Character 创建了数值在 [0,127] 范围的缓存数据，Boolean直接返回True或False。

**两种浮点类型的包装类Float、Double并没有实现包装类技术。**

**所有整型包装类对象之间值的比较，全部使用equals方法进行比较。**

我们以 `int与Integer` 作为例子，看下"=="和"equal()"方法：

1）基本型和封装类型进行"=="运算符的比较，封装类型将会自动拆箱变为基本型后再进行比较，因此Integer(0)会自动拆箱为int类型再进行比较。

2）两个Integer类型进行"=="比较，如果其值在-128至127，那么返回true，否则返回false, 这跟Integer.valueOf()的缓冲对象有关，后面会说。

3）两个封装类型进行equals()比较，首先equals()会比较类型，如果类型相同，则继续比较值，如果值也相同，返回true。

4）基本型封装类型调用equals(),但是参数是基本类型，这时候，先会进行自动装箱，基本型转换为其封装类型，再进行3中的比较。

## 为什么要有包装类型？

* Java是一种面向对象编程的语言，它要求所有的东西都是对象。然而，基本数据类型不是对象，它们是原始的数据类型，无法直接参与面向对象中的特性（比如继承、多态等）。因此引入了包装类型，使其能够像其它对象那样参与面向对象。
* **泛型的支持**，Java5引入了泛型，但是泛型要求类型参数必须是对象类型，不能是基本数据类型。因此，如果要在集合类中存储基本数据类型，就需要使用对应的包装类型。
* **空值的支持**，包装类型可以表示空值（null），而基本数据类型不行。
* 包装类型提供了一些额外的方法和功能，使得对基本数据类型的操作更加方便。

## **什么是自动拆装箱？原理？**

## 遇到过自动拆箱引发的NPE（空指针）问题么？

* 数据库的查询结果可能是null，因为自动拆箱，使用基本数据类型进行接受会有NPE风险。
* 在三目运算（b?x:y）中，当 x和y的类型不一致时，会发生自动拆箱，将x,y全部转换为基本类型，在自动拆箱的过程中有可能NPE。

## String，StringBuffer和StringBuilder的区别是什么？String为什么时不可变的？

* 可变性：String是不可变的，StringBuffer和StringBuilder是可变的；

* 线程安全性：String和StringBuffer是线程安全的，可以被多个线程安全的使用，StringBuilder不是线程安全的

* StringBuffer的性能相对较低，因为它的所有公共方法都是同步的，这意味着在多线程环境下会有性能开销。

  StringBuilder的性能优于StringBuffer，但因为它的方法都不是同步的，因此在单线程环境下更有效。

* String实现了serializable接口，可以被序列化。

重载和重写的区别？**

* 重载指的是在同一个类中，可以定义多个方法具有相同的名称但具有不同的参数列表（参数类型、参数个数、参数顺序）；

  重写指的是子类重新定义（覆盖）了父类中具有相同名称、参数列表和返回类型的方法。

* 重载方法可以有不同的返回类型，但不能仅仅通过返回类型的不同来区分方法。

* 重写方法具有与被重写方法相同的方法签名，包括方法名、参数列表和返回类型。

* 重写方法不能降低访问权限，即子类中重写的方法的访问权限不能比父类中被重写的方法的访问权限更低。

* 重载是编译时多态，重写是运行时多态。

* 重写方法一定不能抛出新的检查异常，或者比被重写方法声明更加宽泛的检查型异常。

**总结：方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载；重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的参数列表，有兼容的返回类型，比父类被重写方法更好访问，不能比父类被重写方法声明更多的异常（里氏代换原则）。重载对返回类型没有特殊的要求，不能根据返回类型进行区分。**

## “==”和“equals ()”的区别

* 对于`==`，比较的是值是否相等

  * 如果作用于基本数据类型的变量，则直接比较存储的值是否相等
  * 如果作用于引用类型的变量，则比较的是所指向的对象的地址是否相等

  （其实==比较的不管是基本数据类型还是引用类型，比较的都是值，只是引用类型变量存储的是对象的地址。）

* 对于`equals()`方法，根据是否重写equals方法，分为两种情况：

  * **类没有重写equals方法：**和==的效果一样，都是比较引用类型的变量所指向的对象的地址。
  * **类重写了equals方法：**比较两个对象的属性是否相等，如果它们的属性相等，则返回true（即，认为这两个对象相等。）
  * **特别注意的是，equals()方法不能作用于基本数据类型的变量。**

## Java反射？反射有什么优缺点？你是怎么理解反射的（为什么框架需要反射？）

* 反射机制是在运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意对象，都能调用它的任意一个方法。在Java中，只要给定类的名字，就可以通过反射机制来获得类的所有信息。这种动态获取的信息以及动态调用对象的方法的功能称为java的反射机制。
* 反射机制的优缺点：
  * 优点：能够运行时动态获取类的实例，提高灵活性和通用性；可以使用反射编写更加通用的代码，减少组件之间的耦合；反射是实现动态代理机制的基础。
  * 缺点：使用反射基本是解释执行，对执行速度有影响。
* 对于反射的理解（为什么框架需要反射）：
  * 通过反射，框架可以在不了解具体类结构的情况下，实现对用户代码的调用和管理，从而提高了框架的灵活性和可扩展性。
  * 依赖注入：在依赖注入中，框架通常需要动态的创建和注入依赖对象，而这些对象的类型通常是在运行时才确定的。通过反射，框架可以实现自动化的依赖注入，无需显式地在代码中指定对象的类型。
  * AOP：框架通常使用动态代理和反射机制来实现AOP，以在运行时动态地植入切面逻辑。
  * 框架通常需要处理大量的配置信息和元数据，以实现各种功能和行为。通过反射，框架可以动态地解析和处理配置文件和元数据，从而实现对各种不同情况的适应性。

## 谈谈对Java注解的理解，解决了什么问题？

在写程序时，有时候想给一些特殊的地方加上标记，告诉编译器或者其他工具，这些地方有一些特殊的意义。Java注解就是一种在代码中添加特殊标记的方式，这些标记可以帮助编译器、工具或者其他程序理解你的代码，并可以触发一些额外的处理或行为。

* 注解解决的问题：
  * 注解可以用于为代码提供更多的文档化信息，这样做可以增加代码的可读性和可维护性，使得代码更容易理解和使用。
  * 注解可以用于在编译时对程序进行动态检查和验证，从而减少运行时错误。
  * 注解可以用于驱动代码生成和自动化处理工具，从而减少重复性工作和提高开发效率（如通过编写自定义的注解，可以在编译时根据注解生成代码、配置文件或其他资源）
  * 注解可以用于配置和扩展框架和库的行为，，使得它们更加灵活和可定制（如在类、方法或字段上添加注解配置框架的行为，以实现依赖注入，AOP等功能）

## Java泛型了解么？泛型的作用？什么是类型擦除？泛型有哪些限制？介绍一下常用的通配符？

* Java 泛型是一种在**编译时**进行类型检查和类型安全的机制，它允许程序员在编写代码时指定类、接口和方法可以支持任意类型（包括类、接口和原始类型）。Java 泛型提供了参数化类型的概念，使得代码可以更加通用和类型安全（可读性更好，更安全）。
* Java 泛型的主要目的是**在编译时捕获类型错误**，并在编译时发现这些错误，从而避免在运行时抛出类型转换异常。它提供了编译时类型安全检查，使得代码更加健壮、可读性更高，并且减少了强制类型转换的需要。
* Java 中的泛型是通过类型擦除实现的，这意味着在编译时会将泛型信息擦除掉，以确保与旧版 Java 代码的兼容性。这也导致了一些泛型相关的限制和局限性。（在运行时无法获取泛型类型的信息，在运行时，数组的元素类型必须是具体的非泛型类型）。
* 泛型的限制：
  * 类型擦除：这意味着在编译后，泛型信息会被擦除掉，因此在运行时无法获取泛型类型的具体信息，这限制了对泛型类型的操作和反射行为。
  * 无法创建参数化类型的数组：例如`new ArrayList<String>[]  ` 是非法的。这是因为在运行时无法获取泛型类型的信息。
  * 在静态上下文（如静态方法或静态初始化块）中无法引用泛型类型的类型参数：因为类型参数是实例化时才确定的，而静态成员在类加载时就已经存在，此时无法确定类型参数的具体值。
  * 不能使用基本类型作为类型参数。
  * 无法实例化泛型类型的具体化对象。
  * 无法创建泛型数组列表。
  * 不能捕获泛型类型的异常：泛型类型不能用作catch块中的异常类型。
  * 不能实例化泛型类型的数组。

12. ### 内部类了解么？匿名内部类了解么？

    * 内部类：如果A类需要直接访问B类中的成员，而B类又需要建立A类的对象，这时为了方便设计和访问，直接将A类定义在B类中就可以了。A类就称为内部类。

    * 内部类可以直接访问外部类的成员。但是外部类访问内部类时，必须先创建内部类对象。

    * 内部类定义在外部类的成员位置上，可以使用一些成员修饰符如private，static。不同的修饰有不同的情况：

      * 默认修饰符default：直接访问内部类格式：`外部类名.内部类名 = 外部类对象.内部类对象(Outer.Inner in = new Outer.Inner())`。但这种方式不常见，因为定义内部类就是为了封装。想要获取内部类对象通常都需要通过外部类的方法来获取。这样可以对内部类对象进行控制。
      * 私有修饰符private：通常内部类被封装，都会被私有化，因为封装性不让其他程序直接访问。
      * 静态修饰符static：如果内部类被静态修饰，相当于外部类，会出现访问局限性，内部类只能访问外部类中的静态成员。**注意：如果内部类中定义了静态成员，那么该内部类必须是静态的。**

    * 匿名内部类是内部类的简化写法。一般只使用一次就可以使用这种形式。匿名内部类其实就是一个匿名子类对象。想要定义匿名内部类，前提是**内部类必须继承一个父类或者实现一个接口。**

      匿名内部类的格式：`new 父类名&接口名{定义子类成员或者覆盖父类方法}.方法`。

    * 匿名内部类的使用场景：当函数的参数是接口类型引用的时候，如果接口中的方法不超过3个。可以通过匿名内部类来完成参数的传递。（换句话说就是，创建匿名内部类时，该类中封装的方法不要太多，最好两个或者两个以内）

2. ### BIO，NIO，AIO有什么区别？

   * BIO（Blocking I/O）：**同步阻塞**I/O模式，数据的读取和写入必须阻塞在同一个线程内等待其完成。

     *  传统BIO（一请求一应答）：

       <img src="https://jiawei-blog-pictures.oss-cn-beijing.aliyuncs.com/blog_pictures/image-20240228203224835.png" alt="image-20240228203224835" style="zoom: 67%;" />

       要使该IO模型能够同时处理多个客户端请求，就必须使用多线程，也就是说它在接收到客户端连接请求后为每一个客户端创建一个新的线程进行链路处理，处理完成后，通过输出流返回应答给客户端，线程销毁。**这就是一请求一应答**。但是这个连接如果不做任何事情的话就会造成不必要的线程浪费，这种情况可以通过使用**线程池**技术改善。

     * 伪异步I/O：引入了线程池来缓解资源浪费问题，但本质上还是传统BIO同步阻塞那一套。无法从根本上解决问题。

       <img src="https://jiawei-blog-pictures.oss-cn-beijing.aliyuncs.com/blog_pictures/image-20240228203836711.png" alt="image-20240228203836711" style="zoom: 67%;" />

   

   * NIO（New I/O）：NIO是一种**同步非阻塞**的IO模型，NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 

     * NIO提供了与传统BIO模型中的 `Socket` 和 `ServerSocket` 相对应的 `SocketChannel` 和 `ServerSocketChannel` 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。
     * 阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。
     * 对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发。

   * NIO与IO的区别（**阻塞与非阻塞，三大组件**）：

     * NIO是非阻塞的，IO是阻塞的

     * **Buffer**：IO面向IO流（Stream oriented），而NIO面向缓冲区（Buffer oriented）。在面向流的IO中，可以将数据直接写入或者将数据直接读到Stream对象中。虽然Stream也有Buffer开头的扩展类，但是只是流的包装类，还是从流读到缓冲区，而NIO是直接读到Buffer中进行操作。

       在NIO中，所有数据都是用缓冲区处理的，在读取数据时，它直接读缓冲区中的；在写数据时，它从缓冲区中读。任何时候访问NIO中的数据，都是通过缓冲区进行操作。

     * **Channel**：NIO通过Channel（通道）进行读写，通道是双向的，可读也可写。而流的读写是单向的。通道只能和Buffer交互。因为Buffer，通道可以异步的读写。

     * **Selectors**： NIO有选择器，而IO没有。选择器用于使用单个线程处理多个通道，因此，他需要较少的线程来处理这些通道。线程之间的切换对于操作系统来说是昂贵的。因此，为了提高效率使用选择器是有效的。

       <img src="https://jiawei-blog-pictures.oss-cn-beijing.aliyuncs.com/blog_pictures/image-20240228210012986.png" alt="image-20240228210012986" style="zoom:50%;" />

   * AIO（Asynchronous I/O）：AIO也就是NIO 2。在Java7中引入了NIO 2，它是**异步非阻塞**的。异步IO是基于时间和回调机制实现的。AIO是异步IO的缩写，虽然NIO在网络操作中，提供了非阻塞的方法，但是NIO的IO行为还是同步的。

   

# Java集合

## 几种集合的数据结构：

* HashMap：哈希表 + 链表 ，无序，允许键和值为null，线程不安全
* LinkedHashMap： 哈希表 + 双向链表，保持插入顺序或访问顺序。
* TreeMap： 基于红黑树实现的有序Map
* Hashtable：基于哈希表实现的map，线程安全
* concurrentHashMap：基于哈希表实现的线程安全的 Map，支持高并发操作。

# JUC

## volatile关键字：

volatile关键字保证了 **可见性（一个线程对共享变量进行修改后，其他线程能立即看到这个修改）** 和 **有序性（程序执行的顺序与代码的顺序一致）**

**volatile如何保证可见性：**

* 一个变量被声明为volatile时，**线程在写入变量时**， 不会立即把值缓存到寄存器或其他地方，而是**会把值刷新回主内存。**
* 当其他线程**读取到该共享变量**，**会从主存中获取最新值**，而不是使用当前线程的本地内存中的值。

**volatile如何保证有序性：**

* volatile通过**限制指令重排序**（编译器重排序 和 处理器重排序 这两种重排序）来保证有序性的。

* **写操作** 加的屏障是 阻止**上方**其它写操作越过屏障**排到 volatile变量写之下**；

  ![image-20240507210723973](https://jiawei-blog-pictures.oss-cn-beijing.aliyuncs.com/blog_pictures/image-20240507210723973.png)

* **读操作** 加的屏障是阻止 下方其它读操作越过屏障**排到 volatile变量读之上**；

  ![image-20240507210744195](https://jiawei-blog-pictures.oss-cn-beijing.aliyuncs.com/blog_pictures/image-20240507210744195.png)

## synchronized关键字

**（1）synchronized的三种用法：**

* 修饰实例方法：这种方式相当于 synchronized（this对象），进入同步代码块之前需要获得 当前对象实例的锁；
* 修饰静态方法：这种用法会作用于类的所有对象实例，进入同步块之前要获得当前 class的锁；
* 修饰代码块： synchronized(类.class) 表示进⼊同步 代码前要获得 当前 class 的锁；

**（2）synchronized 怎么保持可见性的：**

* 线程枷锁前，清空工作内存中共享变量的值，从而从主存中读取最新的共享变量值；
* 线程加锁后，其他线程无法获取主存中的共享变量；
* 线程解锁前，会将最新的共享变量值刷新到主存中。

**（3）synchronized 怎么保持有序性的：**

* synchronized同步的代码块，一次只能被一个线程拥有，所以synchronized保证**同一时刻，代码是单线程执行的。** **因为as-if-serial语义的存在，单线程的程序能保证最终结果是有序的，但是不保证不 会指令重排。**

**（4）synchronized怎么实现可重入的：**

* synchronized锁对象的时候有一个**锁计数器，它会记录下线程获取锁的次数，在执行完代码块后，计数器 -1，直到计数器清零，就释放锁了。**

# 二、Spring框架

## 1.Bean生命周期

大致可分为四个阶段： **实例化 -> 属性赋值 -> 初始化 -> 销毁**

* **（实例化）**首先，在Bean容器中找到SpringBean的定义BeanDefination。
* **（实例化）**Bean容器根据Java Reflection API创建一个Bean的实例（调用构造方法实例化bean）。
* **（属性赋值）**设置属性（为Bean设置相关属性和依赖、或者说依赖注入）
* **（初始化）**初始化Initialization：
  * 检查Aware的相关接口并设置相关依赖。如果实现了BeanAware接口，调用setBeanName()方法，传入Bean的名字。
  * BeanPostProcessor前置处理；如果实现了 BeanFactoryAware 接⼝，调⽤ setBeanFactory() ⽅法，传⼊ BeanFactory 对 象的实例。 类似，如果实现了其他 *.Aware 接⼝，就调⽤相应的⽅法。
  * 检查是否实现InitializationBean接口；如果 Bean 实现了 InitializingBean 接⼝，执⾏ afterPropertiesSet() ⽅法。
  * 检查是否配置自定义的init-method；如果 Bean 在配置⽂件中的定义包含 init-method 属性，执⾏指定的⽅法。
  * BeanPostProcessor后置处理
* 使用中
* **（销毁）**销毁
  * 是否实现DisposableBean接口
  * 是否配置自定义的destory-method
* 结束。

## 2. Spring循环依赖

* 只有单例Bean才会存在循环依赖。
* 当循环依赖的实例都采用setter方法注入，Spring可以解决；都采用构造器方法注入时，spring不可以解决；构造器注入和setter注入同时存在的时候，看天。

**Spring通过三级缓存解决了循环依赖问题：**

* 一级缓存：singletonObjects**单例池**，用于保存实例化、属性赋值（注入）、初始化完成的bean实例；
* 二级缓存：earlySingletonObjects**早期曝光对象**，用于保存实例化完成的bean实例
* 三级缓存：singletonFactories**早期曝光对象工厂**，用于保存Bean创建工厂，以便于后面扩展有机会创建代理对象。

## 3. Spring事务

### spring事务隔离级别：

* **ISOLATION_DEFAULT： 使用后端数据库的默认隔离级别**，mysql默认 可重复读 级别
* ISOLATION_READ_UNCOMMITTED：即读未提交，最低的隔离级别，不能解决脏读，幻读，不可重复读
* ISOLATION_READ_COMMITTED：即 读已提交，可以解决脏读，不能解决不可重复读 以及 幻读；
* .ISOLATION_REPEATABLE_READ：即 可重复读，可解决脏读，不可重复读，不能解决幻读；
* ISOLATION_SERIALIZABLE： 串行化，完全服从ACID，但是会严重影响性能。

### Spring事务传播行为：

事务传播行为是为了解决**service层方法之间互相调用**的事务问题（一个事务方法被另一个事务方法调用时，需要指定事务如何传播）。

**事务传播行为包括如下：**

* REQUIRED： 默认的事务传播行为，required表示当前方法必须在一个事务中运行，如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务；
* SUPPORTS：当前方法支持在一个事务中运行，如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式执行；
* MANDATORY：当前方法必须在一个事务中运行，如果当前存在事务，则加入该事务，如果当前没有事务，则抛出异常；
* REQUIRES_NEW：当前方法必须在一个新的事务中运行，如果当前存在事务，就挂起该事务，并创建一个新的事务。
* NOT_SUPPORTED：当前方法不应该在事务中运行，如果当前存在事务，就挂起该事务，以非事务的方式执行。
* NEVER：表示当前方法不应在事务中运行，如果当前存在事务，则抛出异常。
* NESTED：表示当前方法必须在一个嵌套的事务中运行，如果当前存在事务，则该在事务的嵌套事务中运行；如果当前没有事务，则创建一个新的嵌套事务。

### 事务失效场景

* **@Transactional 应用在非 public 修饰的方法上**：spring要求被代理方法必须是public的。在`AbstractFallbackTransactionAttributeSource`类的`computeTransactionAttribute`方法中有个判断，如果目标方法不是 public，则`TransactionAttribute`返回 null，即不支持事务。
* **方法用 final修饰：**如果将事务方法定义为final，这样会导致事务失效。原因是spring事务底层使用了aop，也就是通过jdk动态代理或者cglib，帮我们生成了代理类，在代理类中实现事务的功能。**如果某个方法被final修饰了，那么在它的代理类中，就无法重写该方法，而添加事务功能**。 同样的，如果某个**方法是static的**，同样**无法**通过动态代理，**变成事务方法**。
* **同一个类中方法调用，导致事务失效：**方法A调用本地的方法B，但**A没有声明注解事务，但是B声明了**，在**外部调用方法A后，B的事务是不会起作用**的。这还是由于SpringAOP代理造成的，因为只有当**事务方法被当前类以外的代码调用的时候，才会由spring生成的代理对象**来管理。
* **方法所在的类没有被Spring管理：**用spring事务的前提是，对象要被spring管理。

### 事务不回滚场景：

* **设置了错误的传播特性 propagation；**
* **自己吞了异常：**在事务方法中手动try catch了异常，这种情况下事务不会正常回滚。
* **手动抛出了别的异常：**即使开发者没有手动捕获异常，但是抛的异常不正确，spring事务也不会回滚。spring事务，默认情况下只会回滚 runtimeException和 error，对于普通的Exception（非运行时异常），它不会回滚。



# 三、计算机网络

## 1. TCP三次握手 & 四次挥手

**三次握手建立连接**。从client的视角来看：

* **第一次握手**： client发送一个带有`syn（seq = x）`标志的数据包到server端，然后标识自己的状态为**syn_send**已发送状态；
* **第二次握手**：server收到了client发来的消息，于是向client发送 `syn (seq = y) + ack（ack=x+1） `确认信息。client接收到确认信息后，这时候client知道自己的发送和接收都能正常进行，对方的发送和接收都正常；server知道自己能正常接收，对方能正常发送（**但不知道对方能否正常接收，自己能不能正常发送**）；在server发送确认信息后表示自己状态为**syn_recv**已接收状态。
* **第三次握手**：client发送一个`ack (ack = y+1)`标志的数据包，然后client和server端都进入 **establish**链接建立状态。完成三次握手。

**三次握手最重要的目的就是双方确认自己与对方的发送与接收都是正常的。**

- **第一次握手**：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常
- **第二次握手**：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常
- **第三次握手**：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常

**第二次握手传回了ack，为什么 还要传回syn?**

传回发送端所发送的 ACK 是为了告诉客户端：“我接收到的信息确实就是你所发送的信号了”，这表明从客户端到服务端的通信是正常的。回传 SYN 则是为了建立并确认从服务端到客户端的通信。

**四次挥手断开连接**：

* **第一次挥手：** client发送一个`FIN（seq = x）`的数据包到服务端，用来关闭客户端到服务端的数据传送，然后client进入**FIN-Wait-1**状态；
* **第二次挥手**：server收到这个`FIN（seq=x）`的数据包，然后发送一个`ACK（ack = x+1）`的数据包到client，随后server进入**Close-wait**状态，client收到这个数据包后进入**FIN-Wait-2**状态；
* **第三次挥手**：server发送一个`FIN （seq = y）`的数据包到client，请求关闭连接，随后server进入**LAST-ACK**状态；
* **第四次挥手**：client发送一个`ACK (ack=y+1)` 的数据包到server，然后client进入到**TIME-Wait**状态，server端在接收到这个数据包后进入**CLOSE**状态。此时如果client等待 **2*MSL**（一个片段在网络中的最大存活时间）没有收到回复，说明server已经正常关闭，随后client也可以关闭连接了。



# 四、JVM

## 1. JVM内存

### jvm内存组成：

* **方法区**（线程共享区）
  * 也称为永久代，JDK8后改为**元空间**；
  * 存储**已加载的类**的类信息、常量、静态变量、即时编译器编译后的代码等；
* **堆**（线程共享区）
  * 存储**对象实例**和**数组**；
  * JVM管理的最大一块内存区域（垃圾回收）
  * 在JVM启动时被创建，并且在运 行时进行动态扩展或收缩；
* **本地方法栈**（线程私有区）
  * 与虚拟机栈类似，但它是为了**本地方法**服务
  * 本地方法使用其他语言写的一些方法，需要特殊的栈支持。
* **虚拟机栈（**线程私有区）
  * 每个线程在执行Java方法时，都会创建一个对应的**栈帧**
  * **栈帧**用来存储**局部变量、方法参数、操作数栈、动态链接**等信息
* **程序计数器**（线程私有区）
  * 每个线程都有自己的程序计数器，用来记录当前线程执行的位置。



### 对象创建过程：

* 遇到new字节码指令的时候，检查指令的参数能否找到一个类的符号引用
* 如果这个符号引用对应的类还没有被加载、解析、初始化过，那么先执行类加载过程。
* 类加载完成后，为新对象分配内存，分配内存的方法包括**指针碰撞、空闲列表**方法。
* 分配完内存，对分配好的内存（除了对象头）进行初始化工作，都初始化为0值。
* 接下来设置对象头，对象头包括所属的类、哈希码、GC分代年龄等。
* 最后，JVM执行对象的构造函数，对对象的实例变量进行初始化或其他必要操作。

### 对象的内存布局

* 对象头
  * 对象运行时数据：
    * 哈希码
    * GC分代年龄
    * 锁状态标识
    * 线程持有的锁
    * 偏向线程ID
    * 偏向时间戳
  * 类型指针（自己代表哪个类）。
  * 如果是数组，还有一个保存自己长度信息的数据
* 实例化数据，存储对象真正的有效信息。
* 对齐填充padding，无实际意义。

### 对象有哪几种引用类型

* 强引用
  * 当一个对象被强引用时，不会被垃圾回收
  * 没有任何强引用指向一个对象时，可以被垃圾回收
* 软引用
  * 相对弱化的强引用，当内存不足时，可能会回收软引用关联的对象
* 弱引用
  * 相对弱化的强引用，无论内存是否充足，都会回收被弱引用关联的对象
* 虚引用
  * 最弱化的强引用，任何时候都可能会被回收，不会通过虚引用获取到对象实例。

### 堆内存分区

* 新生代
  * Eden区（较大）
  * survivor区 （较小）
    * from区
    * to区
* 老年代

对象被创建时，会被放至Eden区，垃圾回收后，Eden中的存活对象会被转入survivor区，经过多次垃圾回收后会转入老年代。老年代的回收频率较低。

每次分配内存只是用Eden和其中一块survivor，发生垃圾收集时，eden和survivor中存活的对象一次性复制到另一块survivor上。然后直接清理掉 Eden和已用过的那块Survivor空间。

### 垃圾回收算法

* **标记-清除法**：标记出要回收的对象，然后回收所有被标记的对象。
  * 缺点：执行效率不稳定，标记和清除两个过程的执行效率都随对象数量增长而降低；并且会造成内存空间的碎片化问题
* **标记-复制法**：划分为大小相等的两块，每次只用一块，当这一块的内存用完了，将存活的内存复制到另一块，这一块的内存直接回收
  * 缺点：始终有一半的内存是空闲的，造成了空间的浪费
  * 所以新生代垃圾回收一般使用这个方法，因为新生代**存活对象**较少
* **标记-整理法**： 让所有存活的对象都向内存空间一端移动，然后直接清理掉边界之外的内存。
  * 缺点：移动存活对象是个极为负重的工作，并且还要stop the word
  * 因此老年代主要用这种方法。

### 类生命周期

1.  加载： 将类的字节码文件加载到内存中，并创建该类的.Class对象
2.  验证： 验证字节码文件是否符合JVM规范
3. 准备： 为类的静态变量分配内存空间，并设置初始值
4. 解析：这一阶段主要是将符号引用转换为直接引用。
5. 初始化：执行构造器方法的过程，这一阶段是类主动使用的阶段。
6. 使用： 在该类被加载和初始化后，就可以使用该类创建对象，调用类的方法。
7. 卸载：当一个类不再被引用，并且也没有活跃的实例时，JVM会卸载该类。

### 类加载过程

1. 通过类的全限定名 来获取此类的二进制字节流
2. 将这个字节流代表的 静态数据结构 转变为 方法区的运行时数据结构
3. 在内存区生成一个 代表这个类的 java.lang.Class 对象，作为 方法区这个类的各种数据的访问入口。

### 类加载器

* **启动类加载器**：又称为根类加载器，是JVM的一部分，负责加载java的核心类库。
* **扩展类加载器**：用来加载java的扩展库，JVM的实现会提供一个扩展库目录，该类加载器在该目录里查找并加载扩展类。
* **系统类加载器**：根据java应用的类路径来加载类，一般java应用的类都是它加载的。可以通过ClassLoader.getSystemClasssLoader() 来获取
* **用户类加载器：**用户可以通过集成java.lang.classLoader的方式自行实现类加载器。

### 双亲委派机制

当一个类加载器收到加载类的请求时，会首先请求父类加载器进行类加载，如果加载成功则类加载结束。否则由子类类加载器进行加载。

### 使用双亲委派机制的好处

* **避免类的重复加载**：如果父类加载器已经加载了一个类，那么子类不会再次加载。**节省了内存**。
* **确保类的安全性和一致性**：Java核心类库由 启动类加载器 加载，可以**防止恶意代码替换核心类库**，保证核心类库的安全性和一致性。
* **防止类的篡改**： 父类加载器加载的类不会被子类加载器替换，防止了类的篡改。
* **方便类加载器的扩展和自定义**： 可以通过扩展classLoader类来实现类加载器的自定义，通过双亲委派机制，可以**方便的扩展和自定义类加载器的功能，满足不同的加载需求。**

### 如何破坏类加载机制

* 自定义类加载器： 可以重写LoadClass方法，实现自己的类加载逻辑，不再委托给父类加载器。但是自定义类加载器需要遵循一定的类加载规则，以确保类加载的一致性和正确性。
* 使用Thread.currentThread.setContextClassLoader方法：通过该方法设置当前线程的上下文类加载器，将类加载的委派顺序改变，这样可以绕过双亲委派机制，直接指定类加载器进行加载。



# 五、MySQL

## 0.基础

### 一条SQL执行的步骤：

* **权限检查：**检查用户是否有执行该语句的权限。
* **查询缓存（仅适用于8.0版本之前）：**如果缓存中有该语句的结果，就从缓存中返回结果。
* **语法分析和语义检查：**由MySQL**解析器**验证语句的合法性。
* **查询优化**：MySQL的**查询优化器**根据查询的复杂度、索引情况、统计信息等因素选择最佳的查询计划。
* **执行查询计划：**涉及表扫描、索引的匹配、数据的过滤和排序等操作。
* **数据检索和返回：** **执行器**根据查询计划获取数据，并将结果返回客户端。如果涉及查询大量数据，MySQL可能会使用临时表来存储中间结果，并进行排序和分页操作。

## 1. 索引

### 索引分类: 

* 按**数据结构**分类：B+树索引、Hash索引、Full-text索引
* 按**物理存储**分类：聚簇索引（主键索引）、二级索引
* 按**字段特性**分类：主键索引、普通索引、唯一索引、前缀索引
* 按**字段个数**分类：唯一索引、联合索引。

### B+树索引（主键索引 or 二级索引）

* 多叉树、叶子节点才存放数据、非叶子节点只存放索引
* 每个节点中的数据是按照主键顺序存放的。
* 叶子节点之间通过双向链表连接。
* 对主键索引来说，叶子节点存放的就是实际数据。
* 对二级索引来说，叶子节点存放的是主键值，而不是实际数据。**回表就是指**需要查询二级索引树和主键索引树两个B+树才能查到数据。
* 当查到的数据在二级索引的叶子节点就能找到时，此时就**不用再回表查询**了。**这被称为覆盖索引**
* 相比于B树，B+树更适合于范围查询，因为其叶子节点用双向链表相连；而且B树的非叶子节点也要存储数据，B+树单节点数据量更小。
* 相比于二叉树，B+树能保证高度始终在3-4层左右，对IO更友好。
* 相比于Hash，查询复杂度o(1)优于B+树，但hash不适合范围查询。

**B+树索引的优势**：相比于B树和二叉树来说，最大的优势在于查询效率高，即使在数据量很大的情况下，**磁盘的IO次数依然维持在3-4次**。

### 联合索引 & 索引下推优化

* **联合索引的最左匹配原则：在遇到范围查询（如 >、<）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。注意，对于 >=、<=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配**
* **索引下推优化：**
  * 对于联合索引（a, b），在执行 `select * from table where a > 1 and b = 2` 语句的时候，只有 a 字段能用到索引，那在联合索引的 B+Tree 找到第一个满足条件的主键值（ID 为 2）后，还需要判断其他条件是否满足（看 b 是否等于 2），那是在联合索引里判断？还是回主键索引去判断呢？
  * 在 MySQL 5.6 之前，只能从 ID2 （主键值）开始一个个回表，到「主键索引」上找出数据行，再对比 b 字段值。
  * 而 MySQL 5.6 引入的**索引下推优化**（index condition pushdown)， **可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数**。
* **建立联合索引时，要把区分度大的字段排在前面，这样区分度大的字段越有可能被更多的 SQL 使用到**。
* MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比（惯用的百分比界线是"30%"）很高的时候，它一般会忽略索引，进行全表扫描。

### ⭐索引失效的情况

* 使用**左或者左右模糊匹配**的时候，会造成索引失效；
* 当在**查询条件中对索引做了计算、函数、类型转换操作**，这些情况下都会造成索引失效；
* 在**联合索引**中，没有遵循**最左匹配原则**，导致索引失效；
* 在 **WHERE 子句**中，如果在 **OR 前**的条件列是**索引列**，而在 **OR 后**的条件列**不是索引列**，那么索引会失效。
* **使用like "%xxx”,索引一定会失效么？**：
  * 如果数据表中的字段**只有主键+二级索引**，那么即使使用了左模糊匹配，也不会走全表扫描，而是**走全扫描二级索引树**。
  * 我们都知道联合索引要遵循最左匹配才能走索引，但是**如果数据表中的字段都是索引**的话，即使查询过程中，**没有遵循**最左匹配原则，也是会**走全扫描二级索引树**。

### 优化索引的方法

* 前缀索引优化
* 覆盖索引优化
* 主键索引最好是自增的
* 防止索引失效

### 索引存储结构

* 根据物理存储分类，索引分为**聚簇索引和二级索引**。
  * 对于**InnoDB**引擎，聚簇索引的叶子节点存放的是实际数据，而二级节点存放的是主键值，而不是实际数据。
  * 对于**MyISAM**引擎，聚簇索引的叶子节点存放的是具体数据的地址。
* MySQL的**默认存储引擎**是**InnoDB**，它采用**B+树**作为索引数据结构。
* **MyISAM 存储引擎**支持多种索引数据结构，比如 B+ 树索引、R 树索引、Full-Text 索引。MyISAM 存储引擎在创建表时，创建的主键索引默认使用的是 **B+ 树索引**。
* 虽然，I**nnoDB 和 MyISAM 都支持 B+ 树索引**，但是它们数据的存储结构实现方式**不同**:
  * **InnoDB** 存储引擎：B+ 树索引的叶子节点保存**数据本身**；
  * **MyISAM 存**储引擎：B+ 树索引的叶子节点保存数据的**物理地址；**
* 对聚簇索引的理解：
  * 聚簇索引具有唯一性：由于聚簇索引是将索引和数据放在一块，所以一个表仅有一个聚簇索引。
  * （1）聚簇索引默认是主键。（2）如果表中没有定义主键，InnoDB会选择第一个唯一的非空索引代替。（3）如果没有这样的索引，InnoDB 会隐式定义一个主键作为聚簇索引。

### MySQL 表的物理结构

* MySQL的**表数据**是以**页**的形式存放的，页在磁盘中**不一定是连续的**。
* 页空间是16K，并不是所有的空间都是用来存放数据的，会有一些固定的信息，如页头、页尾、页码、校验码等。
* 在B+树中，叶子节点和非叶子节点的数据结构是一样的，区别在于，**叶子节点**存放的是**实际的行数据**，而**非叶子节点**存放的是**主键和页号。**

<img src="https://jiawei-blog-pictures.oss-cn-beijing.aliyuncs.com/blog_pictures/image-20240429195215797.png" alt="image-20240429195215797" style="zoom: 67%;" />



* 索引结构不会影响单表最大行数，2000W也只是推荐值，**超过了这个值可能会导致B+树层级更高，影响查询性能**。

## 2.  事务

### 2.1 事务隔离级别

#### **四个隔离级别：**

* 读未提交（啥都解决不了）

* 读已提交 （解决了脏读）

* 可重复读 （解决了脏读，不可重复读，InnoDB默认的隔离级别）

* 串行化 （解决了脏读，不可重复读以及幻读，但是效率最低，不轻易使用。）

* InnoDB 引擎的**默认隔离级别是 可重复读**，但是它很大程度上避免了幻读现象。解决的方案有两种：
  * 针对**快照读（普通select）**，是通过**MVCC**方式解决了幻读。因为可重复读隔离级别下，事务执行**过程中看到的数据**，一直**跟**这个**事务启动时看到的数据是一致的**，即使中途有其他事务插入了一条数据，是查询不出来的。
  * 针对**当前读（select ... for update等语句）**，是通过**next-key lock(记录锁 + 间隙锁)**方式解决了幻读。因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被**阻塞**，无法成功插入，所以就很好了避免幻读问题。
  
* InnoDB引擎为了解决 **可重复读** 隔离级别使用 **当前读** 而造成的**幻读**问题，引出了**间隙锁**（**即使这样也只是最大程度避免幻读，没有完全解决幻读**）

  * 表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。

  * 事务 A 执行了这面这条锁定读（for update）语句后，就在对表中的记录加上 id 范围为 (2, +∞] 的 next-key lock（next-key lock 是间隙锁+记录锁的组合）。![image-20240430153508438](https://jiawei-blog-pictures.oss-cn-beijing.aliyuncs.com/blog_pictures/image-20240430153508438.png)

    然后，事务 B 在执行插入语句的时候，判断到插入的位置被事务 A 加了 next-key lock，于是**事物 B 会生成一个插入意向锁**，同时**进入等待**状态，**直到**事务 **A 提交了事务**。这就**避免了由于事务 B 插入新记录而导致事务 A 发生幻读**的现象。

    

####  四个隔离级别是如何实现的

* 对于 **读未提交** 级别，实现方式相对简单，因为可以读到未提交事务修改的数据，所以**直接读取最新的数据就好**了。
* 对于 **读已提交** 和 **可重复读** 级别来说，他们是**通过read view来实现**的，它们的区别在于：**读已提交**是在 **每个语句执行前** 都会重新生成 read view，而**可重复读** 是在 **启动事务时** 生成一个read view，然后整个事务期间都在使用这个read view
* 对于 **串行化** 级别，通过添加**读写锁（排他锁）**的方式实现。

## 3.  MySQL锁

### **全局锁**：

* 使用全局锁后，整个数据库就处于只读状态了。这时其他线程执行以下操作，都会被阻塞：
  * 对**数据的增删改**操作，比如 insert、delete、update等语句；
  * 对**表结构的更改**操作，比如 alter table、drop table 等语句。
* 全局锁主要用于做**全库逻辑备份**，这样在备份期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。

* 如果备份花费很长时间，在这个时间段内业务只能读数据，造成业务的停滞。
* 如果数据库的引擎支持 **可重复读** 的隔离级别，那么在**备份数据库之前先开启事务**，会先创建read view ,然后整个事务执行期间都在使用这个read view，而且由于MVCC的支持，**备份期间业务依然可以对数据进行更新操作**。（**InnoDB引**擎默认的隔离级别正是可重复读，**因此可采用这种方式备份库**，而 **MyISAM不支持事务**，备份库时就**只能使用全局锁**）

### 表级锁

**表级锁有这几种：**

* 表锁
* 元数据锁 （MDL）
* 意向锁
* Auto-INC 锁

**（1） 表锁**

* 表锁除了会限制别的线程读写之外，也**会限制本线程接下来的读写操作**。
* 尽量**避免在使用InnoDB 引擎的表使用表锁**，因为表锁的颗粒度太大，影响并发性能，**InnoDB 实现了粒度更细的行级锁，更牛逼。**

**（2）元数据锁**

* **不用显式**的使用，因为我们在对数据库表进行操作时，会自动给这个表加上MDL；
  * 对**表进行crud**的时候，添加的时**MDL读锁；**
  * 对**表结构**进行变更的时候，添加的是**MDL写锁**；

* MDL在事务提交之后才会释放，这意味着**事务执行期间，MDL是一直持有的。**
* 申请MDL锁的操作会形成一个队列，队列中，**写锁的优先级高于读锁**，一旦出现MDL写锁等待，**会阻塞后续该表中所有的CRUD操作**（因为拿不到读锁了）。

**（3）意向锁**

* 在使用 InnoDB 引擎的表里对某些记录加上**「共享锁」**之前，需要先在表级别加上一个「**意向共享锁」**；
* 在使用 InnoDB 引擎的表里对某些纪录加上**「独占锁」**之前，需要先在表级别加上一个**「意向独占锁**」；

**意向锁是表级锁**，**不会和行级**的共享锁和独占锁发生**冲突**，而且**意向锁之间也不会发生冲突**，只会和共享**表锁**和独占**表锁**发生冲突

**意向锁的目的是为了快速判断表里是否有记录被加锁，**如果表被上了意向独占锁，表明表内有数据被加了独占锁，这样其他线程就不用再去逐条看是哪一条记录被添加了独占锁。

**（4）Auto-INC 锁**

- 设置主键自增时，再插入数据时，可以不指定主键的值，数据库会自动给主键赋值递增的值，这主要是**通过Auto_INC锁实现**的。
- **auto-inc是一种特殊的表锁机制**，**锁不再是一个事务提交之后才释放，而是在执行完插入语句之后就会立即释放。**
- 在插入数据时，会添加一个**表级别**的Auto-INC锁，然后给自增的字段赋值，等**插入语句执行完**后，才会把Auto-Inc**释放。**
- **InnoDB 提供了一个轻量级锁**，插入时，会给自增字段加上轻量级锁，然后给该字段赋值自增，之后就释放了，**不用再等插入语句结束后再释放。**

### 行级锁（只有InnoDB 支持）

**普通的select语句是不会对记录进行加锁的**，因为它属于快照读。如果要在查询时对记录加锁，可以使用下面这种方式，查询加锁的方式称为**锁定读**：

* `select ... lock in share mode (加共享锁)`
* `select ... for update（加独占锁）`

**行级锁有四类：**

- **Record lock**，记录锁，也就是仅锁一条记录。
  - 记录锁有 **共享锁** 和 **独占锁** 之分。如果一个事务对当前记录加了共享锁，那么其他事务可以加共享锁但是不能加独占锁；如果是加了独占锁，那么其他事务既不能加独占锁也不能加共享锁。
- **Gap Lock**， 间隙锁，锁定一个范围，但是不包含记录本身，**只存在于可重复读隔离级别**；目的是为了解决该级别下 **幻读**的现象。
  - **间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的**
- **Next-Key Lock** , 是上面两种锁的组合，**锁定一个范围，并且锁定记录本身。**
- **插入意向锁：** 名字虽然有意向锁，但是它并**不是意向锁，它是一种特殊的间隙锁，属于行级锁。**
  - 当一个事务想插入一条记录，但是发现插入位置有其他事务设置的间隙锁，那么当前插入操作会阻塞，直到拥有这些间隙锁的事务提交为止。
  - 在此期间，该事务会生成一个插入意向锁，表明有业务向再这个区间插入新纪录，但是现在在等待。

## 4. 日志

### MySQL的三个日志：

* undo log：回滚日志，是InnoDB**存储引擎层**生成的日志，实现事务的**原子性**。主要用于**事务回滚和MVCC**。
* redo log：重做日志，是InnoDB**存储引擎层**生成的日志，实现事务的**持久性**，主要用于**掉电等故障恢复**。
* binlog：二进制日志，是**server层**生成的日志，主要用于**数据备份和主从复制**（可以认为是为了实现事务的**一致性**）。

### 关于undo log:

* **（作用1）**undo log即回滚日志，用于在事务提交失败的时候进行回滚操作时使用，用于实现**原子性**。
* 每当InnoDB引擎对一条记录进行 **增删改** 的时候（这时候会**自动隐式的开启事务**），需要把回滚时需要的信息记录到 undo log 中：
  * 插入时，记录插入记录的主键值，回滚时删除就好；
  * 删除时，记录该记录的所有内容，回滚时插入到表内就好；
  * 更新时，记录被更新列的旧值，回滚时恢复就好。
* **（作用2）**undo log的另一个作用是：**通过readview + undo log来实现MVCC**（多版本并发控制）：
  * 首先引入**版本链**：每条记录有两个隐藏列 **trx_id（该记录是被哪个事务修改的）** 和 **roll-pointer （一个指针，可以将这些undo log 串成一个链表，这个链表称为版本链）**
  * **读已提交** 和 **可重复读** **两个隔离级别**，他们的**快照读**都是通过read view + undo log来实现的，区别在与**创建readview的时机不同：**
    * 读已提交是在每个select都会生成一个read view；
    * 可重复读是在启动事务时生成一个read view，整个事务期间都在使用这个read view;
  * 两个隔离级别是通过 **事务的read view里的字段** 和 **记录中的两个隐藏列** 进行比对，如果不满足可见，就会顺着 undo log版本链找到满足其可见性的记录，从而控制并发事务访问同一个记录的行为。这就叫**MVCC**

### 关于buffer pool:

引入buffer pool是为了提升读写性能，buffer pool中缓存的是数据页（默认16kb）：

* 如果读记录时，该记录在缓冲池中，那么就从缓冲池中读，否则去磁盘中读；
* 修改记录时，如果记录存在于缓冲池中，那么直接修改该记录在缓冲池中的页，并将其标志为脏页。脏页不会立即写入磁盘，后台线程会挑选一个合适的时机将其写入磁盘。

MySQL启动的时候，InnoDB会为 buffer pool中申请一片空闲空间，随着程序运行，磁盘上的页会逐渐放进这些空闲空间中。

当查询一条记录的时候，InnoDB是会把整个页的数据加载到buffer pool中，页加载后，再通过页中的 页目录 去定位到某条具体的记录。

### 关于redo log:

引入redo log，是因为**buffer pool是基于内存**的，在断电重启时，**没来得及落盘的脏页数据会丢失**。为了解决这个问题，**当有一条记录需要修改的时候，InnoDB首先更新内存（标记为脏页），然后将本次对这个页的修改以 redo log的形式记录下来。**

随后，后台线程会在合适的时机将脏页写到磁盘中。

* redo log 是**物理日志**，记录了**某个数据页做了什么修改**，比如**对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新**，每当执行一个事务就会产生这样的一条或者多条物理日志。
* 事务提交时，只用**先将redo log持久化到磁盘**中就行，（内存中的脏页是在合适的时机进行落盘的）。**哪怕系统崩溃发生内存掉电重启，根据磁盘中redo log中的记录进行恢复即可**。

### redo log 和 undo log的区别

* 这俩日志都是InnoDB引擎的日志；
* redo log是重做日志，记录了此次事务完成后的状态，记录的是更新后的值；
* undo log是回滚日志，记录了此次事务开始前的状态，记录的是更新前的值；
* 如果事务提交之前发生崩溃，会通过undo log回滚事务；如果事务提交之后发生崩溃，会通过redo log恢复事务。
* redo log保证了持久性；而undo log保证了原子性；

### bin log与 redo log的区别

1. **适用对象不同：**

* bin log是**server层**实现的日志，**所有存储引擎都能用**。
* redo log和 undo log都是**InnoDB引擎**实现的日志。

2. **文件格式不同：**

* （**记录逻辑操作**）bin log 有3种格式类型，分别是**statement(默认的格式)、row、mixed**，区别如下：
  * StateMent：每一条修改数据的sql都会记录到bin log中（相当于记下了逻辑操作，可以称为逻辑日志）。主从复制中 slave 根据sql语句重现。但是**有动态函数的问题**，比如UUID或now，在主库中执行的结果并不是从库中执行的结果。**这会导致复制的数据不一致。**
  * Row： 记录行数据最终被变成什么样了，不会出现动态函数的问题。但它的**缺点是每行数据的变化结果都会被记录**，比如批量执行update , statement模式就只会记录一个update语句，而 row模式下，有多少次update 就产生多少update语句，使得binlog文件过大。
  * mixed: 包含了上面的 statement 和 row 模式，他会根据不同的情况自动使用row模式和statement模式。
* **redo log是物理日志**，记录某个记录在某个数据页做了什么修改。

3. **写入方式不同**：

* **bin log是追加写**，写满一个文件就创建一个新的文件接着写，不会覆盖旧日志，**保留的是全量日志**；
* **redo log是循环写**，日志空间大小是固定的，全部写满就从头开始，保**存未被刷入磁盘的脏页日志**；

4. **用途不同:**

* bin log用于备份恢复，主从复制（一致性）；
* redo log用于掉电等故障恢复 （持久性）；

5. **写入时机不同**：

* bin log在**事务提交之后**才会被写入磁盘，因此可能会有一定的延迟；
* redo log在**事务提交之前**就会被写入磁盘，确保了事务的持久性；

**如果不小心把整个数据库的数据删除了，那么需要使用bin log来进行恢复，因为bin log保留了全量日志，这也是为什么redo log不行。**

###  主从复制如何实现（依赖bin log）：

复制的过程就是将**bin log中的数据从主库传输到从库上**。这个过程一般是**异步**的。

**MySQL 集群的主从复制过程**分为三个阶段：

* **写入 bin log**： 主库写 bin log，提交事务，并更新本地存储的数据；
* **同步 bin log**：把bin log复制到所有从库上，每个从库把bin log写到relay log暂存日志中，返回给主库一个 复制成功 的响应；
* **回放 bin log**：回放 bin log，并更新存储引擎中的数据：从库创建一个用于回放 bin log的线程，去读relay log中继日志，然后回放bin log更新数据，最终实现主从的数据一致性。

**主从复制模型**：

* 同步复制：很烂，主库要所有从库都完成复制才能返回客户端的结果，性能很差。
* **异步复制**：默认的模型，这种模式下，**一旦主库发生宕机，数据就会发生丢失**。
* **半同步复制**：5.7版本后增加的一种方式，事务线程不用等待所有从库复制成功响应，只**要一部分复制成功响应回来就行。**这种方式兼顾了同步复制和异步复制的有点，**即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。**

### 为什么需要进行两阶段提交？

在**持久化**redo log和bin log这两份日志的时候，**如果出现半成功的状态**（两个log有一个持久化成功了，一个没成功），就**会造成主从库内容的不一致性。**这是因为**redo log影响主库的内容，bin log影响从库的内容，所以redo log和bin log必须保持一致才能保证主从一致性。**

**半成功就是下面两种情况：**

* **redo log持久化成功了（事务成功提交了），但是bin log没有持久化成功**：这导致主库恢复后根据redo log 有最新的数据，但是bin log会丢失更新，导致主从复制后，从库里保存的也不是最新的数据。
* **bin log持久化成功了，但是redo log没有持久化成功（redo log没持久化成功，那么恢复时会认为该事务无效）**：因为恢复后MySQL认为该事务失效，所以恢复后直接回滚，主库保留的还是旧值。但是bin log里有最新的数据，在主从复制后又会导致不一致问题。

**==两阶段提交的内容==**

两阶段提交把单个事务的提交分为两个阶段： **准备阶段（prepare）和 提交阶段（commit）**阶段：

* 在准备阶段，  协调者 询问两个 参与者（也就是这两个日志）有没有准备好。参与者 会做出应答，表示自己准备好了，或者没有准备好。
* 在提交阶段，如果都准备好了，那么协调者 就可以执行 两个日志的持久化过程；如果有任何一个参与者没有准备好，协调者会宣布事务失效，执行事务的回滚操作。

**具体内容概括如下：**

* **准备阶段：将redo log对应的事务提交状态设置为prepare，然后将redo log刷新到硬盘；**
* **提交阶段：将bin log刷新到磁盘，接着调用引擎的提交事务接口，将redo log状态设置为commit**（将事务设置为commit状态后，把该状态刷入到磁盘 redo log文件）；



# Redis

## 如何保证缓存和数据库的一致性：

首先确定一点，在保证可用性和分区容错性的前提下，**无法保证缓存和数据库的绝对一致**。

### **产生不一致的两种原因**如下：

* 缓存key删除失败：没删除成功，但是数据库确实更新了，然后就不一致了；
* 并发写入了脏数据：如果我们先删除缓存，再更新数据库。那么并发过程中，一部分线程会读到还没更新好的数据库中的旧数据，之后用这个旧数据重建缓存，就导致了缓存和数据库的不一致性。

关于缓存更新策略，这里选用 删除缓存而不是更新缓存 的方式，因为删除缓存比更新缓存快得多。

整体的缓存更新策略就是：**先删缓存，再更新数据库。**但是这里除了这两步，还会有其他的策略尽可能保证一致性。

### 缓存更新策略：

1. **消息队列保证key被删除**：

通过引入消息队列，**把要删除的key 或者 删除失败的key 丢进消息队列**，**利用消息队列的重试机制，重试删除对应的key**；

这个方法的缺点在于，对业务代码有一定的侵入性。

2. **数据库订阅+消息队列保证key被删除：**

可以**用一个服务去监听数据库的 binlog，获取需要操作的数据。然后用一个公共的服务获取订阅程序传来的信息，进行缓存删除**操作。

这种方式降低了对业务的侵入，但是提升了系统的复杂度。

3. **⭐延迟双删防止脏数据**

在第二条产生不一致的原因中，缓存可能读到没更新的旧数据然后进行缓存重建，这就导致了脏数据的出现。这种情况的解决方式就是延迟双删。

**先删除缓存 ---> 更新数据库 ---> 隔一段时间再次删除缓存**

这种方式下，**延时时间需要仔细考量和测试**。

4. **设置缓存过期时间兜底**

朴素但有用的方法，给缓存设置一个合理的过期时间，即使发生了不一致，也不会永远不一致下去。

## 怎么处理热key：

**热key就是 redis中被频繁访问的key**。当某个key被大量的读取或写入操作访问时，就会导致这个key成为热key。

### 如何对热key进行监控：

* **客户端监控**：例如，可以在客户端（指Redis客户端）设置全局字典，每次调用redis命令时，使用这个字典进行记录。
* **代理端监控：**像Twemproxy、Codis这些基于代理的redis分布式架构，所有客户端的请求都是通过代理端完成的，可以在代理端进行收集统计。
* **Redis服务端监控：**monitor命令可以监控到Redis执行的所有命令，因此可以用这个指令统计热点key。

### 热key处理方法：

* **数据分片**：将热key分散到不同的redis节点上，以避免某个节点的负载过高，提高整个系统的性能。
* **使用二级缓存：**在热key的处理中，可以引入二级缓存，将热key的数据提前加载到内存中。当redis宕机时，可以直接从内存中查询数据。

### 热key重建（联系缓存击穿）：

缓存重建需要耗费比较长的时间，可能涉及sql查询、IO、复杂的计算等。**在缓存失效的瞬间，大量线程会同时发起缓存重建的请求，导致后端压力增大甚至崩溃**。

针对热点key重建问题，采用以下方案来优化：

* 使用互斥锁：用锁来保证只有一个线程能够缓存重建，其他线程等待重建缓存的线程执行完毕，再从缓存中读取数据即可。
* 设置永不过期：热点key在缓存中不设置过期时间，同时为每个value设置一个逻辑过期时间，当发现逻辑时间过期后，用一个单独的线程来进行重建缓存。这种方式可以在保证一致性的情况下，减少重建缓存的次数。

## 过期数据回收策略：

* **惰性删除**：当访问到一个过期的key的时候，redis会立即删除该key。
  * 优点：如果过期键经常被访问，能及时删除过期键。
  * 缺点：如果过期键很少被访问，会占用大量内存。
* 定时删除： 在设置key的过期时间时，同时创建一个定时事件，当事件到达时，由事件处理器自动执行key的删除操作。
* **定期删除**：redis默认每10秒执行一次定时任务，随机抽取一部分键进行检查，删除过期的键。
  * 这种策略通过定期删除过期键来释放内存，相比于惰性删除，可以更及时的删除过期键，但也有可能会有一些过期键没被删除。

## 内存溢出控制/内存淘汰策略：

redis所用内存达到**maxmemory**上限时会触发相应的溢出控制策略。

### 溢出控制策略：

1. **不进行数据淘汰的策略：**

* noeviction：当内存不足，不会淘汰任何数据，而是直接报错。
  * 适用于对数据丢失要求比较高的场景，但有可能OOM或者redis进程被杀死。

2. **进行数据淘汰的策略** （又分为 **在设置了过期时间的数据中淘汰** 和 **在所有数据范围内淘汰**）：

   2.1**在设置了过期时间的数据中淘汰的：**

   * volatile-LRU：淘汰**最近最少使用的、带有过期时间** 的数据。适用于缓存一些热点数据的场景，但可能导致一些长期未使用的数据被误删。
   * volatile-random：随机淘汰设置了过期时间的任意键值。
   * volatile-ttl：优先淘汰更早过期的值。
   * volatile-LFU：淘汰所有设置了过期时间的键值中，最少使用的键。

   2.2**在所有数据范围内进行淘汰的策略：**

   * allkeys-LRU：淘汰最近最少使用的数据，**不管这些数据是否过期**。会导致有用的数据被误删。
   * allkeys-random：随即淘汰任意键值。
   * allkeys-LFU：淘汰整个键范围中最少使用的键。

3. **LFU 和 LRU的区别：**

   * LRU是最近最少使用，redis在对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间。进行内存淘汰时，redis随机选取5个值，然后淘汰最久未被使用的那个。
   * LFU是最近最不常用的意思，LFU会记录数据的访问次数，然后根据访问次数来淘汰数据。

